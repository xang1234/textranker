{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# How TextRank Works: A Visual Explanation\n",
    "\n",
    "This notebook provides a step-by-step visual explanation of the TextRank algorithm. We'll trace through each stage of the algorithm to understand how keywords are extracted.\n",
    "\n",
    "**Algorithm Overview:**\n",
    "1. **Tokenization** - Break text into words with POS tags\n",
    "2. **Co-occurrence Graph** - Build a graph connecting nearby words\n",
    "3. **PageRank** - Iteratively compute word importance scores\n",
    "4. **Phrase Extraction** - Group high-scoring words into keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q rapid_textrank matplotlib networkx numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in stopwords used by the native tokenizer\n",
    "import rapid_textrank as rt\n",
    "\n",
    "stopwords = rt.get_stopwords(\"en\")\n",
    "print(f\"Built-in stopwords (en): {len(stopwords)}\")\n",
    "if len(stopwords) > 30:\n",
    "    print(stopwords[:30])\n",
    "else:\n",
    "    print(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from rapid_textrank import BaseTextRank, PositionRank, BiasedTextRank, TextRankConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our example text\n",
    "text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables\n",
    "systems to learn and improve from experience. Deep learning, a type of\n",
    "machine learning, uses neural networks with many layers.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1: Tokenization\n",
    "\n",
    "The first step is to break the text into tokens and identify their parts of speech. TextRank focuses on **content words** (nouns, verbs, adjectives) while filtering out stopwords and function words.\n",
    "\n",
    "Above, we print the built-in stopword list used by the native tokenizer (you can extend it via config).\n",
    "\n",
    "Let's simulate what rapid_textrank's built-in tokenizer produces:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated tokenization (rapid_textrank does this internally in Rust)\n",
    "# We'll show a simplified version for educational purposes\n",
    "\n",
    "tokens = [\n",
    "    # Sentence 1\n",
    "    {\"text\": \"Machine\", \"lemma\": \"machine\", \"pos\": \"NOUN\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    {\"text\": \"learning\", \"lemma\": \"learning\", \"pos\": \"NOUN\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    {\"text\": \"is\", \"lemma\": \"be\", \"pos\": \"AUX\", \"sentence_idx\": 0, \"is_stopword\": True},\n",
    "    {\"text\": \"a\", \"lemma\": \"a\", \"pos\": \"DET\", \"sentence_idx\": 0, \"is_stopword\": True},\n",
    "    {\"text\": \"subset\", \"lemma\": \"subset\", \"pos\": \"NOUN\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    {\"text\": \"of\", \"lemma\": \"of\", \"pos\": \"ADP\", \"sentence_idx\": 0, \"is_stopword\": True},\n",
    "    {\"text\": \"artificial\", \"lemma\": \"artificial\", \"pos\": \"ADJ\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    {\"text\": \"intelligence\", \"lemma\": \"intelligence\", \"pos\": \"NOUN\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    {\"text\": \"that\", \"lemma\": \"that\", \"pos\": \"PRON\", \"sentence_idx\": 0, \"is_stopword\": True},\n",
    "    {\"text\": \"enables\", \"lemma\": \"enable\", \"pos\": \"VERB\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    {\"text\": \"systems\", \"lemma\": \"system\", \"pos\": \"NOUN\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    {\"text\": \"to\", \"lemma\": \"to\", \"pos\": \"PART\", \"sentence_idx\": 0, \"is_stopword\": True},\n",
    "    {\"text\": \"learn\", \"lemma\": \"learn\", \"pos\": \"VERB\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    {\"text\": \"and\", \"lemma\": \"and\", \"pos\": \"CCONJ\", \"sentence_idx\": 0, \"is_stopword\": True},\n",
    "    {\"text\": \"improve\", \"lemma\": \"improve\", \"pos\": \"VERB\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    {\"text\": \"from\", \"lemma\": \"from\", \"pos\": \"ADP\", \"sentence_idx\": 0, \"is_stopword\": True},\n",
    "    {\"text\": \"experience\", \"lemma\": \"experience\", \"pos\": \"NOUN\", \"sentence_idx\": 0, \"is_stopword\": False},\n",
    "    # Sentence 2\n",
    "    {\"text\": \"Deep\", \"lemma\": \"deep\", \"pos\": \"ADJ\", \"sentence_idx\": 1, \"is_stopword\": False},\n",
    "    {\"text\": \"learning\", \"lemma\": \"learning\", \"pos\": \"NOUN\", \"sentence_idx\": 1, \"is_stopword\": False},\n",
    "    {\"text\": \"a\", \"lemma\": \"a\", \"pos\": \"DET\", \"sentence_idx\": 1, \"is_stopword\": True},\n",
    "    {\"text\": \"type\", \"lemma\": \"type\", \"pos\": \"NOUN\", \"sentence_idx\": 1, \"is_stopword\": False},\n",
    "    {\"text\": \"of\", \"lemma\": \"of\", \"pos\": \"ADP\", \"sentence_idx\": 1, \"is_stopword\": True},\n",
    "    {\"text\": \"machine\", \"lemma\": \"machine\", \"pos\": \"NOUN\", \"sentence_idx\": 1, \"is_stopword\": False},\n",
    "    {\"text\": \"learning\", \"lemma\": \"learning\", \"pos\": \"NOUN\", \"sentence_idx\": 1, \"is_stopword\": False},\n",
    "    {\"text\": \"uses\", \"lemma\": \"use\", \"pos\": \"VERB\", \"sentence_idx\": 1, \"is_stopword\": False},\n",
    "    {\"text\": \"neural\", \"lemma\": \"neural\", \"pos\": \"ADJ\", \"sentence_idx\": 1, \"is_stopword\": False},\n",
    "    {\"text\": \"networks\", \"lemma\": \"network\", \"pos\": \"NOUN\", \"sentence_idx\": 1, \"is_stopword\": False},\n",
    "    {\"text\": \"with\", \"lemma\": \"with\", \"pos\": \"ADP\", \"sentence_idx\": 1, \"is_stopword\": True},\n",
    "    {\"text\": \"many\", \"lemma\": \"many\", \"pos\": \"ADJ\", \"sentence_idx\": 1, \"is_stopword\": True},\n",
    "    {\"text\": \"layers\", \"lemma\": \"layer\", \"pos\": \"NOUN\", \"sentence_idx\": 1, \"is_stopword\": False},\n",
    "]\n",
    "\n",
    "# Display as a table\n",
    "print(f\"{'Token':<15} {'Lemma':<15} {'POS':<8} {'Stopword':<10} {'Content Word'}\")\n",
    "print(\"-\" * 70)\n",
    "for t in tokens:\n",
    "    is_content = t['pos'] in ['NOUN', 'VERB', 'ADJ', 'PROPN'] and not t['is_stopword']\n",
    "    marker = \"\u2713\" if is_content else \"\"\n",
    "    print(f\"{t['text']:<15} {t['lemma']:<15} {t['pos']:<8} {str(t['is_stopword']):<10} {marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only content words (these will form graph nodes)\n",
    "content_words = [t for t in tokens if t['pos'] in ['NOUN', 'VERB', 'ADJ', 'PROPN'] and not t['is_stopword']]\n",
    "\n",
    "print(f\"Content words ({len(content_words)} total):\")\n",
    "for i, w in enumerate(content_words):\n",
    "    print(f\"  {i+1:>2}. {w['lemma']} ({w['pos']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2: Co-occurrence Graph\n",
    "\n",
    "TextRank builds an **undirected graph** where:\n",
    "- **Nodes** = Content words (lemmas)\n",
    "- **Edges** = Words that appear near each other (within a sliding window)\n",
    "\n",
    "The `window_size` parameter controls how far apart words can be to still be connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_graph(tokens, window_size=3):\n",
    "    \"\"\"Build a co-occurrence graph from tokens.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Filter to content words only\n",
    "    content_words = [t for t in tokens if t['pos'] in ['NOUN', 'VERB', 'ADJ', 'PROPN'] and not t['is_stopword']]\n",
    "    \n",
    "    # Add all nodes\n",
    "    for word in content_words:\n",
    "        G.add_node(word['lemma'])\n",
    "    \n",
    "    # Add edges for words within window (same sentence)\n",
    "    for i, word_i in enumerate(content_words):\n",
    "        for j in range(i + 1, min(i + window_size, len(content_words))):\n",
    "            word_j = content_words[j]\n",
    "            # Only connect words in the same sentence\n",
    "            if word_i['sentence_idx'] == word_j['sentence_idx']:\n",
    "                lemma_i, lemma_j = word_i['lemma'], word_j['lemma']\n",
    "                if G.has_edge(lemma_i, lemma_j):\n",
    "                    G[lemma_i][lemma_j]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(lemma_i, lemma_j, weight=1)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Build graph with default window size\n",
    "G = build_cooccurrence_graph(tokens, window_size=3)\n",
    "\n",
    "print(f\"Graph Statistics:\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the co-occurrence graph\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Use spring layout for nice visualization\n",
    "pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "\n",
    "# Draw edges with width proportional to weight\n",
    "edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "nx.draw_networkx_edges(G, pos, width=[w * 2 for w in edge_weights], \n",
    "                       alpha=0.6, edge_color='gray', ax=ax)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue', \n",
    "                       edgecolors='darkblue', linewidths=2, ax=ax)\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold', ax=ax)\n",
    "\n",
    "# Draw edge labels (weights)\n",
    "edge_labels = {(u, v): G[u][v]['weight'] for u, v in G.edges()}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8, ax=ax)\n",
    "\n",
    "ax.set_title(\"Co-occurrence Graph (window_size=3)\", fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "window-size-header",
   "metadata": {},
   "source": [
    "### Effect of Window Size\n",
    "\n",
    "The `window_size` parameter affects how connected the graph is:\n",
    "- **Smaller window** (2-3): Only very close words are connected\n",
    "- **Larger window** (6-8): More connections, captures broader relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window-size-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "window_sizes = [2, 4, 8]\n",
    "\n",
    "for ax, ws in zip(axes, window_sizes):\n",
    "    G = build_cooccurrence_graph(tokens, window_size=ws)\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    nx.draw_networkx_edges(G, pos, width=[w * 2 for w in edge_weights], \n",
    "                           alpha=0.6, edge_color='gray', ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=1200, node_color='lightblue', \n",
    "                           edgecolors='darkblue', linewidths=2, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold', ax=ax)\n",
    "    \n",
    "    ax.set_title(f\"window_size={ws}\\n({G.number_of_edges()} edges)\", fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Effect of Window Size on Co-occurrence Graph\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3: PageRank\n",
    "\n",
    "PageRank is an iterative algorithm that distributes \"importance\" through the graph:\n",
    "\n",
    "1. Start with equal scores for all nodes\n",
    "2. Each iteration: redistribute scores based on connections\n",
    "3. Repeat until scores converge (change less than threshold)\n",
    "\n",
    "The **damping factor** (default 0.85) controls how much score \"leaks\" vs. stays in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pagerank-iteration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_with_history(G, damping=0.85, max_iterations=100, tol=1e-6):\n",
    "    \"\"\"Run PageRank and track score evolution.\"\"\"\n",
    "    N = len(G)\n",
    "    if N == 0:\n",
    "        return {}, []\n",
    "    \n",
    "    # Initialize equal scores\n",
    "    scores = {node: 1.0 / N for node in G.nodes()}\n",
    "    history = [dict(scores)]  # Track each iteration\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_scores = {}\n",
    "        for node in G.nodes():\n",
    "            # Sum contributions from neighbors\n",
    "            rank_sum = 0\n",
    "            for neighbor in G.neighbors(node):\n",
    "                neighbor_degree = G.degree(neighbor)\n",
    "                if neighbor_degree > 0:\n",
    "                    rank_sum += scores[neighbor] / neighbor_degree\n",
    "            \n",
    "            # Apply damping factor\n",
    "            new_scores[node] = (1 - damping) / N + damping * rank_sum\n",
    "        \n",
    "        # Check convergence\n",
    "        diff = sum(abs(new_scores[n] - scores[n]) for n in G.nodes())\n",
    "        scores = new_scores\n",
    "        history.append(dict(scores))\n",
    "        \n",
    "        if diff < tol:\n",
    "            break\n",
    "    \n",
    "    return scores, history\n",
    "\n",
    "# Run PageRank\n",
    "G = build_cooccurrence_graph(tokens, window_size=3)\n",
    "scores, history = pagerank_with_history(G)\n",
    "\n",
    "print(f\"PageRank converged in {len(history)-1} iterations\")\n",
    "print(f\"\\nFinal scores (sorted by importance):\")\n",
    "for word, score in sorted(scores.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {word:<15} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pagerank-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score evolution over iterations\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot each word's score over time\n",
    "iterations = range(len(history))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(G.nodes())))\n",
    "\n",
    "for (word, color) in zip(sorted(G.nodes()), colors):\n",
    "    word_scores = [h[word] for h in history]\n",
    "    ax.plot(iterations, word_scores, label=word, color=color, linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Iteration\", fontsize=12)\n",
    "ax.set_ylabel(\"PageRank Score\", fontsize=12)\n",
    "ax.set_title(\"PageRank Score Evolution\", fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graph-with-scores",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize graph with node size proportional to score\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "G = build_cooccurrence_graph(tokens, window_size=3)\n",
    "pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "\n",
    "# Node sizes based on PageRank scores\n",
    "node_sizes = [scores[node] * 15000 for node in G.nodes()]\n",
    "\n",
    "# Edge weights\n",
    "edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "nx.draw_networkx_edges(G, pos, width=[w * 2 for w in edge_weights], \n",
    "                       alpha=0.5, edge_color='gray', ax=ax)\n",
    "\n",
    "# Draw nodes with size = importance\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', \n",
    "                       edgecolors='darkblue', linewidths=2, ax=ax)\n",
    "\n",
    "# Labels with scores\n",
    "labels = {n: f\"{n}\\n({scores[n]:.3f})\" for n in G.nodes()}\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=9, ax=ax)\n",
    "\n",
    "ax.set_title(\"Co-occurrence Graph with PageRank Scores\\n(node size = importance)\", \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damping-header",
   "metadata": {},
   "source": [
    "### Effect of Damping Factor\n",
    "\n",
    "The damping factor controls how much importance is distributed vs. retained:\n",
    "- **Low damping (0.5)**: More uniform distribution, less differentiation\n",
    "- **High damping (0.95)**: More influenced by graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damping-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_values = [0.5, 0.85, 0.95]\n",
    "\n",
    "print(\"Effect of Damping Factor:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for d in damping_values:\n",
    "    scores, _ = pagerank_with_history(G, damping=d)\n",
    "    top_words = sorted(scores.items(), key=lambda x: -x[1])[:5]\n",
    "    \n",
    "    print(f\"\\nDamping = {d}:\")\n",
    "    for word, score in top_words:\n",
    "        print(f\"  {word:<15} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4: Phrase Extraction\n",
    "\n",
    "After computing word scores, TextRank extracts **phrases** by:\n",
    "1. Finding sequences of content words\n",
    "2. Aggregating their PageRank scores (sum, mean, max, or RMS)\n",
    "3. Ranking phrases by aggregated score\n",
    "\n",
    "Let's see how rapid_textrank performs the full extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rapid_textrank (the complete algorithm)\n",
    "extractor = BaseTextRank(top_n=10, language=\"en\")\n",
    "result = extractor.extract_keywords(text)\n",
    "\n",
    "print(\"Final Extracted Phrases:\")\n",
    "print(\"=\" * 50)\n",
    "for p in result.phrases:\n",
    "    print(f\"{p.rank:>2}. {p.text:<30} score={p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregation-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare aggregation methods\n",
    "aggregation_methods = [\"sum\", \"mean\", \"max\", \"rms\"]\n",
    "\n",
    "print(\"Score Aggregation Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for method in aggregation_methods:\n",
    "    config = TextRankConfig(score_aggregation=method, top_n=5, language=\"en\")\n",
    "    extractor = BaseTextRank(config=config)\n",
    "    result = extractor.extract_keywords(text)\n",
    "    \n",
    "    print(f\"\\n{method.upper()} aggregation:\")\n",
    "    for p in result.phrases:\n",
    "        print(f\"  {p.text:<30} {p.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positionrank-viz",
   "metadata": {},
   "source": [
    "## PositionRank Visualization\n",
    "\n",
    "PositionRank assigns higher initial weights to words appearing early in the document. Let's visualize these position weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "position-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate position weights (inverse position)\n",
    "content_words = [t for t in tokens if t['pos'] in ['NOUN', 'VERB', 'ADJ', 'PROPN'] and not t['is_stopword']]\n",
    "\n",
    "# PositionRank uses inverse position as weight\n",
    "position_weights = {}\n",
    "for i, word in enumerate(content_words):\n",
    "    lemma = word['lemma']\n",
    "    # Sum inverse positions for repeated words\n",
    "    weight = 1.0 / (i + 1)\n",
    "    position_weights[lemma] = position_weights.get(lemma, 0) + weight\n",
    "\n",
    "# Normalize\n",
    "total = sum(position_weights.values())\n",
    "position_weights = {k: v/total for k, v in position_weights.items()}\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "words = list(position_weights.keys())\n",
    "weights = list(position_weights.values())\n",
    "\n",
    "bars = ax.bar(range(len(words)), weights, color='steelblue', edgecolor='darkblue')\n",
    "ax.set_xticks(range(len(words)))\n",
    "ax.set_xticklabels(words, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel(\"Position Weight\", fontsize=12)\n",
    "ax.set_title(\"PositionRank: Words Get Higher Weight When They Appear Earlier\", \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, weight in zip(bars, weights):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "            f'{weight:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "position-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BaseTextRank vs PositionRank\n",
    "base = BaseTextRank(top_n=5, language=\"en\")\n",
    "pos = PositionRank(top_n=5, language=\"en\")\n",
    "\n",
    "base_result = base.extract_keywords(text)\n",
    "pos_result = pos.extract_keywords(text)\n",
    "\n",
    "print(f\"{'BaseTextRank':<30} {'PositionRank':<30}\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(5):\n",
    "    base_p = f\"{base_result.phrases[i].text} ({base_result.phrases[i].score:.3f})\"\n",
    "    pos_p = f\"{pos_result.phrases[i].text} ({pos_result.phrases[i].score:.3f})\"\n",
    "    print(f\"{base_p:<30} {pos_p:<30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biasedtextrank-viz",
   "metadata": {},
   "source": [
    "## BiasedTextRank Visualization\n",
    "\n",
    "BiasedTextRank gives initial boost to specified focus terms, causing related words to rank higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biased-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text with multiple topics\n",
    "multi_topic_text = \"\"\"\n",
    "Modern web applications must balance user experience with security.\n",
    "Performance optimizations are crucial for mobile users on slow networks.\n",
    "Privacy regulations like GDPR require careful data handling.\n",
    "Security vulnerabilities can expose sensitive user information.\n",
    "\"\"\"\n",
    "\n",
    "focus_terms = [\"security\", \"privacy\"]\n",
    "\n",
    "# Highlight which words are focus terms\n",
    "print(\"Focus terms highlighted in text:\")\n",
    "highlighted = multi_topic_text\n",
    "for term in focus_terms:\n",
    "    highlighted = highlighted.replace(term, f\"**{term.upper()}**\")\n",
    "    highlighted = highlighted.replace(term.capitalize(), f\"**{term.upper()}**\")\n",
    "print(highlighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biased-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare unbiased vs biased\n",
    "base = BaseTextRank(top_n=5, language=\"en\")\n",
    "biased = BiasedTextRank(\n",
    "    focus_terms=[\"security\", \"privacy\"],\n",
    "    bias_weight=5.0,\n",
    "    top_n=5,\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "base_result = base.extract_keywords(multi_topic_text)\n",
    "biased_result = biased.extract_keywords(multi_topic_text)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Unbiased\n",
    "texts = [p.text for p in base_result.phrases]\n",
    "scores = [p.score for p in base_result.phrases]\n",
    "colors = ['steelblue'] * len(texts)\n",
    "axes[0].barh(range(len(texts)), scores, color=colors, edgecolor='darkblue')\n",
    "axes[0].set_yticks(range(len(texts)))\n",
    "axes[0].set_yticklabels(texts)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel(\"Score\")\n",
    "axes[0].set_title(\"BaseTextRank (Unbiased)\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# Biased\n",
    "texts = [p.text for p in biased_result.phrases]\n",
    "scores = [p.score for p in biased_result.phrases]\n",
    "# Highlight focus-related terms\n",
    "colors = ['coral' if any(f in t.lower() for f in focus_terms) else 'steelblue' for t in texts]\n",
    "axes[1].barh(range(len(texts)), scores, color=colors, edgecolor='darkblue')\n",
    "axes[1].set_yticks(range(len(texts)))\n",
    "axes[1].set_yticklabels(texts)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel(\"Score\")\n",
    "axes[1].set_title(\"BiasedTextRank (focus: security, privacy)\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"BiasedTextRank Steers Results Toward Focus Terms\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "TextRank is an elegant algorithm that leverages graph theory for keyword extraction:\n",
    "\n",
    "1. **Tokenization**: Extract content words (nouns, verbs, adjectives)\n",
    "2. **Co-occurrence Graph**: Connect words that appear near each other\n",
    "3. **PageRank**: Iteratively compute word importance scores\n",
    "4. **Phrase Extraction**: Group and rank high-scoring words\n",
    "\n",
    "**Variants** modify this basic process:\n",
    "- **PositionRank**: Boosts early-appearing words\n",
    "- **BiasedTextRank**: Steers results toward focus terms\n",
    "\n",
    "**Key parameters** affecting results:\n",
    "- `window_size`: How far apart words can be to connect (default: 4)\n",
    "- `damping`: PageRank damping factor (default: 0.85)\n",
    "- `score_aggregation`: How to combine word scores (sum, mean, max, rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **[04_benchmarks.ipynb](04_benchmarks.ipynb)** - See how rapid_textrank's Rust implementation compares to pytextrank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}